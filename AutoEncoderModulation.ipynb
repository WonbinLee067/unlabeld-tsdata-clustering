{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#이미지 -> 배열화하는 방법은 도저히 일반화시킬 수가 없을 듯함\n",
    "#if _____ in _____ 문법이기 때문에 해당 부분을 모든 데이터에 일반화시킬 수 없을 듯하여 이를 고민해봐야 할 듯 함\n",
    "# learning_rate 적용은 optimizer에 해주었으며, optimizer 부분에 learning_rate 외 여러 파라미터 존재 -> 어떤 부분이 필요하고 어떤 부분이 불필요한지 판단\n",
    "# https://keras.io/ko/optimizers/\n",
    "# 사용자 지정 파라미터 : batch_size, optimizer, batch_size, epochs, loss 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "#from keras.preprocessing import image\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_folder_path = './resources/imgs300x300/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_set(): #해당 카테고리의 경우 이미지 이름에서 카테고리를 분류할 수 있는 이름으로 입력받아야 함\n",
    "    categories = input().split()\n",
    "    print(categories)\n",
    "    \n",
    "    return categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_list(groups_folder_path):\n",
    "    data_path = os.listdir(groups_folder_path)\n",
    "    \n",
    "    X = []\n",
    "    Y = []\n",
    "    categories = category_set()\n",
    "    \n",
    "    for filepath in data_path:\n",
    "        #print(filepath)\n",
    "        filepath = groups_folder_path + filepath\n",
    "        file_path = os.listdir(filepath)\n",
    "        # print(file_path)\n",
    "\n",
    "        for data in file_path:\n",
    "            path_file = filepath + '/' + data\n",
    "            #print(path_file)\n",
    "            # print(data)\n",
    "            # img = img = image.load_img(path_file, target_size=(28, 28,1))\n",
    "            image = Image.open(path_file)\n",
    "            image = image.convert('L')  # 흑백으로 맹글기\n",
    "            image = image.resize((28, 28))\n",
    "            img_data = np.array(image)\n",
    "            img_data = img_data.reshape((28, 28, 1))\n",
    "\n",
    "            #img = cv2.resize(img, None, fx=image_w/img.shape[1], fy=image_h.shape[0])\n",
    "            # img_data = image.img_to_array(img)\n",
    "            # img_data = np.expand_dims(img_data, axis=0)\n",
    "            # img_data = preprocess_input(img_data)\n",
    "            # X.append(np.array(img_data))\n",
    "            img_data_np = np.array(img_data)\n",
    "            \n",
    "            for i in range(len(categories)):\n",
    "                if categories[i] in data:\n",
    "                    X.append(img_data_np)\n",
    "            \n",
    "            for i in range(len(categories)):\n",
    "                if categories[i] in data:\n",
    "                    Y.append(categories[i])\n",
    "                    \n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "                    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(groups_folder_path, X, Y):\n",
    "    X, Y = image_to_list(groups_folder_path)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X,Y)\n",
    "    xy = (X_train, X_test, Y_train, Y_test)\n",
    "    np.save(\"./img_data.npy\", xy)\n",
    "    X_train, X_test, Y_train, Y_test = np.load('./img_data.npy', allow_pickle = True)\n",
    "    \n",
    "    return X_train, X_test, Y_train, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Activation, Dense\n",
    "from keras.layers import Flatten, Convolution2D, MaxPooling2D\n",
    "from keras.models import load_model\n",
    "from keras import optimizers\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, UpSampling2D\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization_tool(data):\n",
    "    data_nor = data.astype('float32') / 255\n",
    "    \n",
    "    return data_nor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "learning_rate = 0.01\n",
    "epochs = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer='Adam'\n",
    "loss='binary_crossentropy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizer_set(optimizer, learning_rate):\n",
    "    if optimizer == 'SGD':\n",
    "        optimizer = optimizers.SGD(lr=learning_rate, clipnorm=1.)\n",
    "        \n",
    "    if optimizer == 'RMSprop':\n",
    "        optimizer = optimizers.RMSprop(lr=learning_rate, rho=0.9, epsilon=None, decay=0.0)\n",
    "        \n",
    "    if optimizer == 'Adam':\n",
    "        optimizer = optimizers.Adam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "        \n",
    "    if optimizer == 'Nadam':\n",
    "        optimizer = optimizers.Nadam(lr=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "        \n",
    "    if optimizer == 'Adagrad':\n",
    "        optimizer = optimizers.Adagrad(lr=learning_rate, epsilon=None, decay=0.0)\n",
    "        \n",
    "    if optimizer == 'Adadelta':\n",
    "        optimizer = optimizers.Adadelta(lr=learning_rate, rho=0.95, epsilon=None, decay=0.0)\n",
    "        \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twentyeightCNNAutoEncoder(groups_folder_path, X, Y, batch_size, learning_rate, epochs, optimizer, loss):\n",
    "    X_train, X_test, Y_train, Y_test = split_data(groups_folder_path, X, Y) #데이터 분리\n",
    "    X_train = normalization_tool(X_train) #데이터 정규화\n",
    "    X_test = normalization_tool(X_test) #데이터 정규화\n",
    "    \n",
    "    optimizer = optimizer_set(optimizer, learning_rate) #optimizer 설정\n",
    "    \n",
    "    input_shape = (28, 28, 1)\n",
    "    model = Sequential()\n",
    "    \n",
    "    # 1st convolution layer\n",
    "    model.add(Conv2D(16, (3, 3)  # 16 is number of filters and (3, 3) is the size of the filter.\n",
    "                 , padding='same', input_shape=(28, 28, 1)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "\n",
    "    # 2nd convolution layer\n",
    "    model.add(Conv2D(2, (3, 3), padding='same'))  # apply 2 filters sized of (3x3)\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "\n",
    "    # here compressed version\n",
    "\n",
    "    # 3rd convolution layer\n",
    "    model.add(Conv2D(2, (3, 3), padding='same'))  # apply 2 filters sized of (3x3)\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(UpSampling2D((2, 2)))\n",
    "\n",
    "    # 4rd convolution layer\n",
    "    model.add(Conv2D(16, (3, 3), padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(UpSampling2D((2, 2)))\n",
    "\n",
    "    model.add(Conv2D(1, (3, 3), padding='same'))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.summary()\n",
    "    \n",
    "    #학습(약 7000번 정도 진행)\n",
    "    model.compile(optimizer=optimizer, loss=loss) #사용자 지정 파라미터(optimizer, loss)\n",
    "    model.fit(X_train, X_train, batch_size = batch_size, epochs=epochs, validation_data=(X_test, X_test)) #사용자 지정 파라미터(epochs)\n",
    "    \n",
    "    X = normalization_tool(X) #데이터 정규화\n",
    "    \n",
    "    compressed_layer = 5\n",
    "    get_3rd_layer_output = K.function([model.layers[0].input],[model.layers[compressed_layer].output])\n",
    "    compressed = get_3rd_layer_output([X])[0]\n",
    "    \n",
    "    #일렬로 늘리기\n",
    "    compressed = compressed.reshape(compressed.shape[0], compressed.shape[1]*compressed.shape[2]*compressed.shape[3])\n",
    "    \n",
    "    return compressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ninetysixCNNAutoEncoder(groups_folder_path, X, Y, batch_size, learning_rate, epochs, optimizer, loss):\n",
    "    X_train, X_test, Y_train, Y_test = split_data(groups_folder_path, X, Y) #데이터 분리\n",
    "    X_train = normalization_tool(X_train) #데이터 정규화\n",
    "    X_test = normalization_tool(X_test) #데이터 정규화\n",
    "    \n",
    "    optimizer = optimizer_set(optimizer, learning_rate) #optimizer 설정\n",
    "    \n",
    "    input_shape = (96, 96, 1)\n",
    "    model = Sequential()\n",
    "    \n",
    "    # 1st convolution layer\n",
    "    model.add(Conv2D(16, (3, 3)  # 16 is number of filters and (3, 3) is the size of the filter.\n",
    "                 , padding='same', input_shape=(96, 96, 1)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "\n",
    "    # 2nd convolution layer\n",
    "    model.add(Conv2D(2, (3, 3), padding='same'))  # apply 2 filters sized of (3x3)\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), padding='same'))\n",
    "\n",
    "    # here compressed version\n",
    "\n",
    "    # 3rd convolution layer\n",
    "    model.add(Conv2D(2, (3, 3), padding='same'))  # apply 2 filters sized of (3x3)\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(UpSampling2D((2, 2), padding='same'))\n",
    "\n",
    "    # 4rd convolution layer\n",
    "    model.add(Conv2D(16, (3, 3), padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(UpSampling2D((2, 2)))\n",
    "    \n",
    "    #5th convolution layer\n",
    "    model.add(Conv2D(2,(3, 3), padding='same')) # apply 2 filters sized of (3x3)\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(UpSampling2D((2, 2)))\n",
    " \n",
    "    #6th convolution layer\n",
    "    model.add(Conv2D(16,(3, 3), padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(UpSampling2D((2, 2)))\n",
    "\n",
    "    model.add(Conv2D(1, (3, 3), padding='same'))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.summary()\n",
    "    \n",
    "    #학습(약 7000번 정도 진행)\n",
    "    model.compile(optimizer=optimizer, loss=loss) #사용자 지정 파라미터(optimizer, loss)\n",
    "    model.fit(X_train, X_train, batch_size = batch_size, epochs=epochs, validation_data=(X_test, X_test)) #사용자 지정 파라미터(epochs)\n",
    "    \n",
    "    X = normalization_tool(X) #데이터 정규화\n",
    "    \n",
    "    compressed_layer = 8\n",
    "    get_3rd_layer_output = K.function([model.layers[0].input],[model.layers[compressed_layer].output])\n",
    "    compressed = get_3rd_layer_output([X])[0]\n",
    "    \n",
    "    #일렬로 늘리기\n",
    "    compressed = compressed.reshape(compressed.shape[0], compressed.shape[1]*compressed.shape[2]*compressed.shape[3])\n",
    "    \n",
    "    return compressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
